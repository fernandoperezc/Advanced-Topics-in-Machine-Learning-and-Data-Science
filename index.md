# Objective

In this seminar, recent papers of the machine learning and data science literature are presented and discussed. Possible topics cover statistical models, machine learning algorithms and its applications.

The seminar “Advanced Topics in Machine Learning and Data Science” familiarizes students with recent developments in machine learning and data science. Recently published articles, as well as influential papers, have to be presented and critically reviewed. The students will learn how to structure a scientific presentation, which covers the motivation, key ideas and main results of a scientific paper. An important goal of the seminar presentation is to summarize the essential ideas of the paper in sufficient depth for the audience to be able to follow its main conclusion, especially why the article is (or is not) worth attention. The presentation style will play an important role and should reach the level of professional scientific presentations.

## Contact:

[Fernando Perez-Cruz](mailto:fernando.perezcruz@sdsc.ethz.ch)

Office TUR E21

## Lectures:

Wednesday 16:00 -- 18:00     LFW  E13

## List of papers:

*   [Attention is all you need.](https://arxiv.org/abs/1706.03762)
*   [Language Models are Unsupervised Multitask Learners.](https://openai.com/blog/better-language-models/)
*   [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.](https://arxiv.org/abs/1810.04805)
*   [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.](https://arxiv.org/abs/2010.11929)
*   [Emerging Properties in Self-Supervised Vision Transformers.](https://arxiv.org/abs/2104.14294)
*   [Pre-trained Transformers As Universal Computation Engines](https://arxiv.org/abs/2103.05247)
*   [Style transfer GAN.](https://arxiv.org/abs/1812.04948)
*   [Variational Inference with Normalizing Flows.](https://arxiv.org/abs/1505.05770)
*   [Moser Flow: Divergence-based Generative Modeling on Manifolds.](https://openreview.net/forum?id=qGvMv3undNJ)
*   [Learning and Generalization in Overparameterized Normalizing Flows.](https://arxiv.org/abs/2106.10535)
*   [Distilling the Knowledge in a Neural Network.](https://arxiv.org/abs/1503.02531)
*   [Symbolic Knowledge Distillation: from General Language Models to Commonsense Models.](https://arxiv.org/abs/2110.07178)
*   [Invariance, Causality and Robustness.](https://arxiv.org/abs/1812.08233)
*   [Causality for Machine Learning.](https://arxiv.org/abs/1911.10500)
*   [Single-Model Uncertainties for Deep Learning.](https://arxiv.org/abs/1811.00908)
*   [A Unified Approach to Interpreting Model Predictions.](https://arxiv.org/abs/1705.07874)
*   [Benchmarking and survey of explanation methods for black box models.](https://arxiv.org/abs/2102.13076)
*   [Learning what and where to attend.](https://arxiv.org/abs/1805.08819)
*   [MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers.](https://openreview.net/forum?id=Tqx7nJp7PR)
*   [Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer.](https://arxiv.org/abs/1808.02651)

## Schedule:

- Feb 23rd: No Lecture
- March 2nd: [Introduction](ATMLDS.pdf)
- March 9th: No lecture
- March 16th:
  - Attention is all you need by Matteo Omenetti. [Slides](Omenetti.pdf)
  - Language Models are Unsupervised Multitask Learners [cancelled].
- March 23rd:
  - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Luca Schweri. [Slides](schweri.pdf)
  - Pre-trained Transformers As Universal Computation Engines by Antonio Lopardo.
- March 30th: 
  - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Jonas Binder. [Slides](Binder.pdf)
  - Emerging Properties in Self-Supervised Vision Transformers by Javier Rando Ramirez. [Slides](Rando.pdf)
- April 6th: 
  - Style transfer GAN by Tianyi Liu. 
  - Variational Inference with Normalizing Flows by Guangda Ji. [Slides](Ji.pdf)
- April 13th:
  - Moser Flow: Divergence-based Generative Modeling on Manifolds by Pascal Mignon. 
  - Learning and Generalization in Overparameterized Normalizing Flows by Yunshu Ouyang.
- April 20th (Easter): No lecture
- April 27th:
  - Causality for Machine Learning by Shanglun Feng. 
  - Invariance, Causality and Robustness by Nora Schneider. 
- May 4th:
  - Benchmarking and survey of explanation methods for black box models by Andri Simeon. 
  - A Unified Approach to Interpreting Model Predictions by Ivan Daniel Rodriguez.
- May 11th:
  - Single-Model Uncertainties for Deep Learning by Gardar Sigurdsson. 
  - Learning what and where to attend by Fiona Muntwyler.
- May 18th:
  - MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers by Kailin Liu. 
  - Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer by Alexey Gavryushin.
- May 25th:
  - Distilling the Knowledge in a Neural Network by Sophie Selgrad. 
  - Symbolic Knowledge Distillation: from General Language Models to Commonsense Models by Junling Wang.
- June 1st: Overflow if needed.

