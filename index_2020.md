# Objective

In this seminar, recent papers of the machine learning and data science literature are presented and discussed. Possible topics cover statistical models, machine learning algorithms and its applications.

The seminar “Advanced Topics in Machine Learning and Data Science” familiarizes students with recent developments in machine learning and data science. Recently published articles, as well as influential papers, have to be presented and critically reviewed. The students will learn how to structure a scientific presentation, which covers the motivation, key ideas and main results of a scientific paper. An important goal of the seminar presentation is to summarize the essential ideas of the paper in sufficient depth for the audience to be able to follow its main conclusion, especially why the article is (or is not) worth attention. The presentation style will play an important role and should reach the level of professional scientific presentations.

## Contact:

[Fernando Perez-Cruz](mailto:fernando.perezcruz@sdsc.ethz.ch)

Office UNH H7

## Lectures:

Wednesday 16:00 -- 18:00     LFW  E 13 and Zoom: https://ethz.zoom.us/j/93919215655

## List of papers:

*   [Integer Discrete Flows and Lossless Compression.](https://papers.nips.cc/paper/9383-integer-discrete-flows-and-lossless-compression.pdf)
*   [Glow: Generative Flow with Invertible 1×1 Convolutions.](https://arxiv.org/pdf/1807.03039.pdf)
*   [Universal Domain Adaptation.](http://openaccess.thecvf.com/content_CVPR_2019/papers/You_Universal_Domain_Adaptation_CVPR_2019_paper.pdf)
*   [Asymmetric Tri-training for Unsupervised Domain Adaptation.](https://arxiv.org/pdf/1702.08400.pdf)
*   [Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces.](https://arxiv.org/pdf/1902.03229.pdf)
*   [Combinatorial Bayesian Optimization using the Graph Cartesian Product.](http://papers.nips.cc/paper/8557-combinatorial-bayesian-optimization-using-the-graph-cartesian-product.pdf)
*   [Variational Inference: A Review for Statisticians.](https://amstat.tandfonline.com/doi/full/10.1080/01621459.2017.1285773#.XiGPCjMo_ds)
*   [Uniform convergence may be unable to explain generalization in deep learning.](http://papers.nips.cc/paper/9336-uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning.pdf)
*   [In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.](https://arxiv.org/pdf/1412.6614.pdf)
*   [The Role of Over-Parametrization in Generalization of Neural Networks.](https://openreview.net/pdf?id=BygfghAcYX)
*   [Fairness Constraints: Mechanisms for Fair Classification.](https://arxiv.org/pdf/1507.05259.pdf)
*   [Overparameterized Neural Networks Can Implement Associative Memory.](https://arxiv.org/pdf/1909.12362.pdf)
*   [Only Bayes should learn  a manifold.](http://www2.compute.dtu.dk/~sohau/papers/onlybayes2018/paper.pdf)
*   [Prescribed Generative Adversarial Networks.](https://arxiv.org/pdf/1910.04302.pdf)
*   [Federated Optimization: Distributed Machine Learning for On-Device Intelligence.](https://arxiv.org/pdf/1610.02527.pdf)
*   [Differentially Private Regression with Gaussian Processes.](http://proceedings.mlr.press/v84/smith18a/smith18a.pdf)
*   [A Fast Learning Algorithm for Deep Belief Nets.](https://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.2006.18.7.1527)
*   [Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks.](https://arxiv.org/pdf/1810.00825.pdf)
*   [Geometric Deep Learning: Going beyond Euclidean data.](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7974879)
*   [Gaussian Process Behaviour in Wide Deep Neural Networks.](https://arxiv.org/pdf/1804.11271.pdf)
*   [A Kernel Two-Sample Test.](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf)

## Schedule:

- Feb 19th: No lecture
- Feb 26th: [Introduction](ATMLDS.pdf)
- March 3rd: No lecture
- March 11th:
  - Fairness Constraints: Mechanisms for Fair Classification: Anian Ruoss
  - The Role of Over-Parametrization in Generalization of Neural Networks: Claudio Ferrari
- March 18th:
  - Variational Inference: A Review for Statisticians: Alicja Chaszczewicz
  - Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces: Noman Ahmed Sheikh
- March 25th:
  - Prescribed Generative Adversarial Networks: Mathieu Chevalley
  - Differentially Private Regression with Gaussian Processes: Athina Nisioti
- April 1st: No lecture
- April 8th:
  - In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning: Michele Marzollo
  - Federated Optimization: Distributed Machine Learning for On-Device Intelligence: Razvan Pasca
- April 22nd:
  - Glow: Generative Flow with Invertible 1×1 Convolutions: Anton Lu
  - Integer Discrete Flows and Lossless Compression: Simon Böhm
- April 29th:
  - Universal Domain Adaptation: Marco Bagatella
  - Overparameterized Neural Networks Can Implement Associative Memory: Matúš Žilinec
- May 6th:
  - Uniform convergence may be unable to explain generalization in deep learning: Cheuk Yu Chan
  - Combinatorial Bayesian Optimization using the Graph Cartesian Product: Pouya Pourjafar Kolaei
- May 13th:
  - Asymmetric Tri-training for Unsupervised Domain Adaptation: Korbinian Abstreiter
  - A Fast Learning Algorithm for Deep Belief Nets: Nicholas Hutchins
- May 20th:
  - Geometric Deep Learning: Going beyond Euclidean data: Guirong Fu
  - Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks: Pavel Senchanka
- May 27th:
  - Gaussian Process Behaviour in Wide Deep Neural Networks: Panuya Balasuntharam
